---
title: "Iris Flower Neural Network"
output: 
  html_document:
    toc: TRUE
    toc_depth: 3
    toc_float: TRUE
    smooth_scroll: TRUE
    css: style.css
    includes: 
      in_header: header.html
      after_body: footer.html
---

# Important Note: 
The code for the Neural Network is completed but putting it into the website is proving trickier than expected. Please hold tight while I fix the issues. I'm trying to get the code embedded as soon as possible. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(shiny)
```
```{python, include=FALSE}
import math
import random
import numpy as np
from datetime import datetime
import time
from sklearn import datasets
import tensorflow as tf
from tensorflow import keras
```

```{python nn, include=FALSE}
#loading this code in at beginning to not lag down the site

class NeuralNetwork():

    def NeuralNetwork(self, inputN, hiddenL, hiddenS, outputN):
        #constructor
        self.input = inputN
        self.hidden = hiddenL
        self.hiddenSize = hiddenS
        self.outputSize = outputN
        self.inputBias = 1
        self.hiddenBias = 1
        self.totalErr = []
    
    def train(self, data, iters, learningRate):
        self.alpha = learningRate
        self.ILweights = [[random.random() for k in range(5)] for q in range(5)]
        #5 branches from each node
        #4 input nodes + 1 bias node
        #5 nodes in the hidden layer + 1 bias
        #3 nodes in output layer, 3 choices
        self.HLweights = [[random.random() for k in range(3)] for q in range(6)]
        #range(3) because 3 branches off
        for i in range(iters):
            info = []
            for inepy in data:
                wah = [self.inputBias]
                for elem in inepy[0].tolist():
                    wah.append(elem)
                tY = [0 for i in range(3)]; tY[inepy[1]] = 1
                inp = [wah, tY]
                t1 = self.feedForward(inp[0])

                HLweighted, output = t1[0], t1[1]
                info.append([inp, HLweighted, output])
                self.backprop(HLweighted, output, inp[0], inp[1], len(data))
            #now all examples have been ff'd
            
    def backprop(self, HLw, pred, x, y, totalEx):
        error = 2*np.subtract(y, pred)
        self.totalErr.append(error)

        
        t = [self.hiddenBias]
        for thing in HLw:
            t.append(thing)
        HLw = t

        ErrSig = np.multiply(error, sig_arr_deriv(pred))
        #Calculating Deriv of 6, HLw and 3, ErrSig
        #Needs to be 6, 3 dims to properly change the HLweights
        DHL = [np.multiply(HLw[i], ErrSig) for i in range(len(HLw))]

        #Calculating DotProd of 5, x (original input) and 6, random maths 
        
        DIL = 0

        self.HLweights += np.multiply(self.alpha, DHL)
        self.ILweights += np.multiply(self.alpha, DIL)
        #print(len(self.HLweights), len(self.HLweights[0]))
        #print(len(DHL), len(DHL[0]))
        #print(len(DIL), len(DIL[0]))
        #print(len(self.ILweights), len(self.ILweights[0]))
        #self.hiddenBias += np.sum(DHL)*self.alpha
        #self.inputBias += np.sum(DIL)*self.alpha

    def feedForward(self, passedIn):
        ret = [0 for i in range(self.outputSize)]
        temp = [0 for i in range(self.hiddenSize)]
        #print(inp, self.ILweights, self.HLweights)

        temp = np.dot(self.ILweights, passedIn)

        for inty in range(len(temp)):
            temp[inty] = sigmoid(temp[inty])

        
        tx = [self.hiddenBias]
        for elemen in temp.tolist():
            tx.append(elemen)
       
        ret = np.dot(tx, self.HLweights)
        
        for minty in range(len(ret)):
            ret[minty] = sigmoid(ret[minty])
        #print(passedIn, tx, ret)
        return [temp, ret]
        #return [temp, ret]

    def test(self, testdata):
        print("-----------------------------------------------------------------")
        tests = 0
        correct = 0
        total = len(testdata)
        DoubleCheck = []
        for x in testdata:
            returned = []
            result = ""
            expected = [0 for i in range(3)]; expected[x[1]] = 1
            returned = self.feedForward(insertBias(self, x[0]))
            ans = returned[1]
            if(ans[0] == expected[0] and ans[1] == expected[1] and ans[2] == expected[2]):
                result = "correct"
                correct+=1
            else:
                result = "wrong"
            tests+=1
            DoubleCheck.append([returned[1], expected])
            maxAns = ans.tolist().index(max(ans))
            maxExp = expected.index(max(expected))
            if(maxAns == maxExp):
                result = "correct"
                correct+=1
            else:
                result="wrong"
            print("Test Number " + str(tests) + ": The Network was " + str(result))
        totalacc=(correct/total)*100
        print("The Network was " + str((correct/total)*100) + "% Correct")
        for element in DoubleCheck:
            print(element)
        return totalacc

def sigmoid(x):
    if(type(x) == int or type(x) == float or type(x) == np.float64):
        return (1 / (1 + np.exp(-1*x)))
    else:
        return(sig_arr(x))

def sig_arr(x):
    return [sigmoid(a) for a in x]

def maxIndex(arr):
    ind = 0
    prevmax = 0
    for i in range(len(arr)):
        if(arr[i] > prevmax):
            ind = i
            prevmax = arr[i]
    return ind

def sig_deriv(x):
    return sigmoid(x)*(np.subtract(1, sigmoid(x)))

def sig_arr_deriv(inarr):
    a = []
    for x in inarr:
        a.append(sig_deriv(x))
    return a

def roundint(x):
    return 1 if (x>=0.5) else 0

def rounder(givArr):
    ret = []
    for x in givArr:
        ret.append(roundint(x))
    return ret

def dot(arr1, arr2):
    rety = []
    for x in arr1:
        rettemp = []
        for y in arr2:
            rettemp.append(x*y)
        rety.append(rettemp)
    return rety

def add(arr, num):
    ret = []
    for mini in arr:
        temp = []
        for x in mini:
            temp.append(x+num)
        ret.append(temp)
    return ret

def mult(arr1, arr2):
    if(len(arr1) == len(arr2)):
        return [arr1[i]*arr2[i] for i in range(len(arr1))]
    else:
        raise Exception("Inequal shapes {} and {}", len(arr1), len(arr2))

#Precondition: given is a default (vanilla i guess) python LIST and not a numpy one
def insertBias(obj, given):
    wah = [obj.inputBias]
    for elem in given:
        wah.append(elem)
    return wah
    
def arrLog(arr):
    return [math.log(abs(x)) for x in arr]

def arrMult(a1, a2):
    while(len(a1) < len(a2)):
        a1.append(1)
    while(len(a2) < len(a1)):
        a2.append(1)
    return np.multiply(a1, a2)
```

# What is the Iris Flower Dataset?

The Iris Flower Dataset, also known as Fisher's Iris set, was introduced by a British scientist named Ronald Fisher in 1936. The Dataset has 50 different samples from each of three different types of flowers - Iris Setosa, Iris Virginica, and Iris Versicolor. Each sample has the sepal length, sepal width, petal length, and petal width of the flower. 

```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('images/sepalpetalimage.png')
```
This dataset can have many uses within data analysis and I decided, as one of my first experiments with Neural Networks and Machine Learning, to build a Neural Network capable of classifying the three different species all the way from the ground up. This meant I wasn't allowed to use any libraries like Numpy or Tensorflow or Keras, even for Linear Algebra. The Network has one input layer with 4 nodes, one hidden layer with 5 nodes, and one output layer with 3 nodes and used the Sigmoid function for the activation of nodes. It took a while to get working, and even now the backpropogation isn't fully working (only the hidden to output layer weights are being modified) but the network ended up with a relatively high accuracy rate on the test dataset. 

# Summary of a Neural Network
A Neural Network is a program designed to mimic how the human brain works. Just think about the process of when you see and identify something - the input layer, or your eyes, takes the information and passes it to the hidden layer, or your brain, which then processes the information and passes the result to the output layer, which is what you identify it as. All the different layers of a Neural Network have "nodes", which are similar to small cells designed to carry information and each node is connected to all the nodes in the next layer. 
Neural Networks generally use two different subsections of the overall dataset: 
+ A training set, which is used (as in its name describes) for training the network
+ A testing set, which is used to test the accuracy of the network after it was trained

```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('images/nnimg.jpeg')
```

# The Network Itself

Now that we've briefly gone over what a neural network is and what its dataset is, we can experiment with it!

The driver code itself is hidden but you'll be able to modify how many iterations the network trains which in turn will affect how accurate (or inaccurate) the network will be when it's tested. 

```{python scratchDriver, include=FALSE, echo=TRUE}
def drive(iters):
  iters = int(iters)
  starting_time = datetime.now().strftime("%H:%M:%S")
  begin = time.process_time()
  print("Time Started: ", starting_time)
  iris = datasets.load_iris()
  irisdat = iris.data
  #print(irisdat)
  numTypes = 3
  #total of 150 different things in the iris dataset
  #4 attributes
  #first 50 are setosa, second 50 are versicolour, last 50 are virginica
  NN = NeuralNetwork()
  NN.NeuralNetwork(len(irisdat[0]), 1, 5, numTypes)
  val = []
  
  for i in range(len(irisdat)):
  
      u = irisdat[i]
      if(i<=50): 
          val.append([u, 0])
      elif(50 < i and i <= 100):
          val.append([u, 1])
      elif(100 < i and i <= 150):
          val.append([u, 2])
  
  random.shuffle(val)
  
  training = val[0:99]
  testing = val[100:]
  NN.train(training, iters, 0.1)
  #print("Previously seen (training) example progress: ")
  #NN.test(training)
  #print("----------------------------------------------")
  print("New and Never Before Seen (testing) example set: ")
  acc = NN.test(testing)
  end_time = time.process_time()
  
  now = datetime.now()
  
  current_time = now.strftime("%H:%M:%S")
  print("Time Started: ", starting_time)
  print("Time Finished: ", current_time)
  print("Time Elapsed: ", (end_time - begin))
  print("Learning Rate: " + str(NN.alpha))
  print("Iterations Trained: " + str(iters))
  return acc
```
```{python, tfdriver, include=FALSE, echo=TRUE}
#Thanks to https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/

def tfnet(iters):
  iters = int(iters)
  iris = datasets.load_iris()
  irisdat = iris.data
  #print(irisdat)
  numTypes = 3
  #total of 150 different things in the iris dataset
  #4 attributes
  #first 50 are setosa, second 50 are versicolour, last 50 are virginica
  val = []

  for i in range(len(irisdat)):
      u = irisdat[i]
      if(i<=50): 
          val.append([u, [1, 0, 0]])
      elif(50 < i and i <= 100):
          val.append([u, [0, 1, 0]])
      elif(100 < i and i <= 150):
          val.append([u, [0, 0, 1]])

  random.shuffle(val)

  training = val[0:99]
  trainX, trainy = np.array([np.array(training[i][0]) for i in range(len(training))]), np.array([np.array(training[i][1]) for i in range(len(training))])
  testing = val[100:]
  testX, testy = np.array([np.array(testing[i][0]) for i in range(len(testing))]), np.array([np.array(testing[i][1]) for i in range(len(testing))])

  alpha = 0.1

  model = keras.Sequential([

      keras.layers.Dense(units=5, activation='sigmoid'), 
      keras.layers.Dense(units=10, activation='sigmoid'),
      keras.layers.Dense(units=3, activation='sigmoid')
  ])

  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(lr=0.05, momentum=0.9), metrics=['accuracy'])
  print("----------- about to start fitting model ------------")
  history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=3000, verbose=0)
  print("--------- about to test predictions ------------")
  test_eval = model.evaluate(testX, testy, verbose=0)
  print("Accuracy on testing set: " + str(test_eval))
  print("Accuracy on training set: " + str(model.evaluate(trainX, trainy, verbose=0)))
  return test_eval
```



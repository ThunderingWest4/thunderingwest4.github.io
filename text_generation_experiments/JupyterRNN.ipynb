{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Text Generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, what this does is use a RNN to train on a dataset from NLTK - specifically a subsection of the Brown dataset from the NLTK package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from string import punctuation\n",
    "import nltk\n",
    "\n",
    "data = \" \".join(nltk.corpus.brown.words('ck10'))\n",
    "tf.compat.v1.enable_eager_execution(\n",
    "    config=None, device_policy=None, execution_mode=None\n",
    ")\n",
    "# Apparently eager execution was turned off on my machine, turned it on\n",
    "seq_len = 100\n",
    "batch_size = 512\n",
    "epochs = 10\n",
    "text = data.lower().translate(str.maketrans(\"\", \"\", punctuation))\n",
    "\n",
    "n_chars = len(text)\n",
    "vocab = sorted(set(text))\n",
    "n_unique = len(vocab)\n",
    "\n",
    "char2int = {c:i for i,c in enumerate(vocab)}\n",
    "int2char = {i:c for i,c in enumerate(vocab)}\n",
    "\n",
    "encoded_text = np.array([char2int[x] for x in text])\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "sequences = char_dataset.batch(2*seq_len + 1, drop_remainder=True)\n",
    "\n",
    "#for seq in sequences.take(2):\n",
    "#    print(''.join([int2char[x] for x in seq.numpy()]))\n",
    "\n",
    "def split_sample(sample):\n",
    "    #Splits a single sample into multiple\n",
    "    ds = tf.data.Dataset.from_tensors((sample[:seq_len], sample[seq_len]))\n",
    "    #print(sample)\n",
    "    for i in range(1, (sample.shape[0]-1)//2):\n",
    "        inp = sample[i:i+seq_len] # sequence starting at i\n",
    "        target = sample[i+seq_len] #value at end of sequence\n",
    "        other = tf.data.Dataset.from_tensors((inp, target))\n",
    "        ds = ds.concatenate(other)\n",
    "    return ds\n",
    "\n",
    "dataset = sequences.flat_map(split_sample)\n",
    "\n",
    "def one_hot(inp, target):\n",
    "    return tf.one_hot(inp, n_unique), tf.one_hot(target, n_unique)\n",
    "\n",
    "dataset = dataset.map(one_hot)\n",
    "\n",
    "#print first two\n",
    "for element in dataset.take(5):\n",
    "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
    "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
    "    print(\"Input shape:\", element[0].shape)\n",
    "    print(\"Target shape:\", element[1].shape)\n",
    "    print(\"=\"*50, \"\\n\")\n",
    "\n",
    "shuffleset = dataset.repeat().shuffle(1024).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(seq_len, n_unique), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256), \n",
    "    Dense(n_unique, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"categorical_crossentropy\")\n",
    "\n",
    "# train the model\n",
    "model.fit(shuffleset, steps_per_epoch=(len(encoded_text) - seq_len) // batch_size, epochs=epochs)\n",
    "# save the model\n",
    "model.save(f\"results/RNNtextgenmodel.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}